{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the code from https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* Set up tensorboard for all functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n",
    "(it's still underfitting at that point, though).\n",
    "'''\n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "from keras.callbacks import CSVLogger, TensorBoard\n",
    "from keras.layers.noise import AlphaDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 1\n",
    "data_augmentation = False\n",
    "num_predictions = 20\n",
    "experiments = 3\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "activations_orig = ['tanh', 'relu', 'linear', 'elu']\n",
    "metrics = {\n",
    "    'orig_relu': {\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'avg_val_loss': 0,\n",
    "        'avg_val_acc': 0\n",
    "    },\n",
    "    'orig_sigmoid': {\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'avg_val_loss': 0,\n",
    "        'avg_val_acc': 0\n",
    "    },\n",
    "    'orig_tanh': {\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'avg_val_loss': 0,\n",
    "        'avg_val_acc': 0\n",
    "    },\n",
    "    'orig_linear': {\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'avg_val_loss': 0,\n",
    "        'avg_val_acc': 0\n",
    "    },\n",
    "    'orig_elu': {\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'avg_val_loss': 0,\n",
    "        'avg_val_acc': 0\n",
    "    },\n",
    "    'orig_selu_rmsp': {\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'avg_val_loss': 0,\n",
    "        'avg_val_acc': 0\n",
    "    },\n",
    "    'orig_selu_adam': {\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'avg_val_loss': 0,\n",
    "        'avg_val_acc': 0\n",
    "    },\n",
    "    'orig_selu_sgd': {\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'avg_val_loss': 0,\n",
    "        'avg_val_acc': 0\n",
    "    },\n",
    "    'rand_all_last_dense': {\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'avg_val_loss': 0,\n",
    "        'avg_val_acc': 0\n",
    "    },\n",
    "    'rand_all_all': {\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'avg_val_loss': 0,\n",
    "        'avg_val_acc': 0\n",
    "    },\n",
    "    'rand_sig_last_dense': {\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'avg_val_loss': 0,\n",
    "        'avg_val_acc': 0\n",
    "    },\n",
    "    'rand_sig_all': {\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'avg_val_loss': 0,\n",
    "        'avg_val_acc': 0\n",
    "    },\n",
    "    'rand_lin_last_dense': {\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'avg_val_loss': 0,\n",
    "        'avg_val_acc': 0\n",
    "    },\n",
    "    'rand_lin_all': {\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'avg_val_loss': 0,\n",
    "        'avg_val_acc': 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with ReLU only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Training for  tanh\n",
      "Experiment 1\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 333s 7ms/step - loss: 1.8056 - acc: 0.3547 - val_loss: 1.5969 - val_acc: 0.4383\n",
      "Saved trained model at c:\\workspace\\activation_functions\\tests\\cifar_10\\saved_models\\cifar10_orig_tanh_0.h5 \n",
      "Test loss: 1.59685731087\n",
      "Test accuracy: 0.4383\n",
      "Average Test loss after 1 experiments: 1.596857\n",
      "Average Test accuracy after 1 experiments: 0.438300\n",
      "------------------------------\n",
      "Training for  tanh\n",
      "Experiment 2\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 334s 7ms/step - loss: 2.1248 - acc: 0.2344 - val_loss: 2.0097 - val_acc: 0.2917\n",
      "Saved trained model at c:\\workspace\\activation_functions\\tests\\cifar_10\\saved_models\\cifar10_orig_tanh_1.h5 \n",
      "Test loss: 2.00973826332\n",
      "Test accuracy: 0.2917\n",
      "Average Test loss after 2 experiments: 1.803298\n",
      "Average Test accuracy after 2 experiments: 0.365000\n",
      "------------------------------\n",
      "Training for  tanh\n",
      "Experiment 3\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 316s 6ms/step - loss: 2.3029 - acc: 0.0980 - val_loss: 2.3028 - val_acc: 0.1000\n",
      "Saved trained model at c:\\workspace\\activation_functions\\tests\\cifar_10\\saved_models\\cifar10_orig_tanh_2.h5 \n",
      "Test loss: 2.30282416725\n",
      "Test accuracy: 0.1\n",
      "Average Test loss after 3 experiments: 1.969807\n",
      "Average Test accuracy after 3 experiments: 0.276667\n",
      "------------------------------\n",
      "Training for  relu\n",
      "Experiment 1\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 299s 6ms/step - loss: 2.3026 - acc: 0.0990 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Saved trained model at c:\\workspace\\activation_functions\\tests\\cifar_10\\saved_models\\cifar10_orig_relu_0.h5 \n",
      "Test loss: 2.30258730011\n",
      "Test accuracy: 0.1\n",
      "Average Test loss after 1 experiments: 2.302587\n",
      "Average Test accuracy after 1 experiments: 0.100000\n",
      "------------------------------\n",
      "Training for  relu\n",
      "Experiment 2\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 287s 6ms/step - loss: 2.3026 - acc: 0.0991 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Saved trained model at c:\\workspace\\activation_functions\\tests\\cifar_10\\saved_models\\cifar10_orig_relu_1.h5 \n",
      "Test loss: 2.30258845062\n",
      "Test accuracy: 0.1\n",
      "Average Test loss after 2 experiments: 2.302588\n",
      "Average Test accuracy after 2 experiments: 0.100000\n",
      "------------------------------\n",
      "Training for  relu\n",
      "Experiment 3\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 285s 6ms/step - loss: 2.3026 - acc: 0.0973 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Saved trained model at c:\\workspace\\activation_functions\\tests\\cifar_10\\saved_models\\cifar10_orig_relu_2.h5 \n",
      "Test loss: 2.30258757439\n",
      "Test accuracy: 0.1\n",
      "Average Test loss after 3 experiments: 2.302588\n",
      "Average Test accuracy after 3 experiments: 0.100000\n",
      "------------------------------\n",
      "Training for  linear\n",
      "Experiment 1\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 275s 6ms/step - loss: 2.3029 - acc: 0.0991 - val_loss: 2.3027 - val_acc: 0.1000\n",
      "Saved trained model at c:\\workspace\\activation_functions\\tests\\cifar_10\\saved_models\\cifar10_orig_linear_0.h5 \n",
      "Test loss: 2.30268270874\n",
      "Test accuracy: 0.1\n",
      "Average Test loss after 1 experiments: 2.302683\n",
      "Average Test accuracy after 1 experiments: 0.100000\n",
      "------------------------------\n",
      "Training for  linear\n",
      "Experiment 2\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 276s 6ms/step - loss: 2.3029 - acc: 0.1002 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Saved trained model at c:\\workspace\\activation_functions\\tests\\cifar_10\\saved_models\\cifar10_orig_linear_1.h5 \n",
      "Test loss: 2.30264840775\n",
      "Test accuracy: 0.1\n",
      "Average Test loss after 2 experiments: 2.302666\n",
      "Average Test accuracy after 2 experiments: 0.100000\n",
      "------------------------------\n",
      "Training for  linear\n",
      "Experiment 3\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 275s 6ms/step - loss: 2.3029 - acc: 0.1001 - val_loss: 2.3029 - val_acc: 0.1000\n",
      "Saved trained model at c:\\workspace\\activation_functions\\tests\\cifar_10\\saved_models\\cifar10_orig_linear_2.h5 \n",
      "Test loss: 2.30286718025\n",
      "Test accuracy: 0.1\n",
      "Average Test loss after 3 experiments: 2.302733\n",
      "Average Test accuracy after 3 experiments: 0.100000\n",
      "------------------------------\n",
      "Training for  elu\n",
      "Experiment 1\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 293s 6ms/step - loss: 2.3029 - acc: 0.0989 - val_loss: 2.3027 - val_acc: 0.1000\n",
      "Saved trained model at c:\\workspace\\activation_functions\\tests\\cifar_10\\saved_models\\cifar10_orig_elu_0.h5 \n",
      "Test loss: 2.30268240433\n",
      "Test accuracy: 0.1\n",
      "Average Test loss after 1 experiments: 2.302682\n",
      "Average Test accuracy after 1 experiments: 0.100000\n",
      "------------------------------\n",
      "Training for  elu\n",
      "Experiment 2\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 292s 6ms/step - loss: 2.3029 - acc: 0.0976 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Saved trained model at c:\\workspace\\activation_functions\\tests\\cifar_10\\saved_models\\cifar10_orig_elu_1.h5 \n",
      "Test loss: 2.30262860222\n",
      "Test accuracy: 0.1\n",
      "Average Test loss after 2 experiments: 2.302656\n",
      "Average Test accuracy after 2 experiments: 0.100000\n",
      "------------------------------\n",
      "Training for  elu\n",
      "Experiment 3\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "28416/50000 [================>.............] - ETA: 1:59 - loss: 2.3028 - acc: 0.1000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-63d1737b766e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                   \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                  verbose=1, callbacks=[csv_logger, tb])\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Save model and weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1657\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for activation in activations_orig:\n",
    "    model_name = 'cifar10_orig_' + activation + '_'\n",
    "\n",
    "    for experiment in range(experiments):\n",
    "        keras.backend.clear_session()\n",
    "        print('-'*30)\n",
    "        print('Training for ', activation)\n",
    "        print('Experiment', experiment+1)\n",
    "\n",
    "    #     model = Sequential()\n",
    "        inputs = Input(shape=(32,32,3))\n",
    "        x = Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:])(inputs)\n",
    "    #     model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "        x = Activation(activation)(x)\n",
    "    #     model.add(Activation('relu'))\n",
    "        x = Conv2D(32, (3, 3))(x)\n",
    "    #     model.add(Conv2D(32, (3, 3)))\n",
    "        x = Activation(activation)(x)\n",
    "    #     model.add(Activation('relu'))\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    #     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        x = Dropout(0.25)(x)\n",
    "    #     model.add(Dropout(0.25))\n",
    "        x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    #     model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "        x = Activation(activation)(x)\n",
    "    #     model.add(Activation('relu'))\n",
    "        x = Conv2D(64, (3, 3))(x)\n",
    "    #     model.add(Conv2D(64, (3, 3)))\n",
    "        x = Activation(activation)(x)\n",
    "    #     model.add(Activation('relu'))\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    #     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        x = Dropout(0.25)(x)\n",
    "    #     model.add(Dropout(0.25))\n",
    "        x = Flatten()(x)\n",
    "    #     model.add(Flatten())\n",
    "        x = Dense(512)(x)\n",
    "    #     model.add(Dense(512))\n",
    "        x = Activation(activation)(x)\n",
    "    #     model.add(Activation('relu'))\n",
    "        x = Dropout(0.5)(x)\n",
    "    #     model.add(Dropout(0.5))\n",
    "        predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    #     model.add(Dense(num_classes))\n",
    "    #     model.add(Activation('softmax'))\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "        # initiate RMSprop optimizer\n",
    "        opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "        # Let's train the model using RMSprop\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=opt,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "        x_train /= 255\n",
    "        x_test /= 255\n",
    "\n",
    "        csv_logger = CSVLogger('../logs/cifar_10/orig_' + activation + '_log_%d.csv' % experiment, append=True, separator=';')\n",
    "        tb = TensorBoard(log_dir='./tb_logs/' + model_name + str(experiment), histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "        model.fit(x_train, y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(x_test, y_test),\n",
    "                  shuffle=True,\n",
    "                 verbose=1, callbacks=[csv_logger, tb])\n",
    "\n",
    "        # Save model and weights\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        model_path = os.path.join(save_dir, model_name + str(experiment) + \".h5\")\n",
    "        model.save(model_path)\n",
    "        print('Saved trained model at %s ' % model_path)\n",
    "        # Score trained model.\n",
    "        scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "        print('Test loss:', scores[0])\n",
    "        print('Test accuracy:', scores[1])\n",
    "\n",
    "        metrics['orig_' + activation]['val_loss'].append(scores[0])\n",
    "        metrics['orig_' + activation]['val_acc'].append(scores[1])\n",
    "\n",
    "        metrics['orig_' + activation]['avg_val_loss'] = sum(metrics['orig_' + activation]['val_loss']) / len(metrics['orig_' + activation]['val_loss'])\n",
    "        metrics['orig_' + activation]['avg_val_acc'] = sum(metrics['orig_' + activation]['val_acc']) / len(metrics['orig_' + activation]['val_acc'])\n",
    "\n",
    "        print('Average Test loss after %d experiments: %f' % (experiment+1, metrics['orig_' + activation]['avg_val_loss']))\n",
    "        print('Average Test accuracy after %d experiments: %f' % (experiment+1, metrics['orig_' + activation]['avg_val_acc'] ))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with SeLU (RMSProp, Adam and SGD optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = ['adam', 'rmsp', 'sgd']\n",
    "\n",
    "for opt in opts:\n",
    "\n",
    "    model_name = 'cifar10_orig_selu_' + opt + '_'\n",
    "\n",
    "    for experiment in range(experiments):\n",
    "        keras.backend.clear_session()\n",
    "        print('-'*30)\n",
    "        print('Experiment', experiment+1)\n",
    "\n",
    "    #     model = Sequential()\n",
    "        inputs = Input(shape=(32,32,3))\n",
    "        x = Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:], kernel_initializer='lecun_normal')(inputs)\n",
    "    #     model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "        x = Activation('selu')(x)\n",
    "    #     model.add(Activation('relu'))\n",
    "        x = Conv2D(32, (3, 3), kernel_initializer='lecun_normal')(x)\n",
    "    #     model.add(Conv2D(32, (3, 3)))\n",
    "        x = Activation('selu')(x)\n",
    "    #     model.add(Activation('relu'))\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    #     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        x = AlphaDropout(0.25)(x)\n",
    "    #     model.add(Dropout(0.25))\n",
    "        x = Conv2D(64, (3, 3), padding='same', kernel_initializer='lecun_normal')(x)\n",
    "    #     model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "        x = Activation('selu')(x)\n",
    "    #     model.add(Activation('relu'))\n",
    "        x = Conv2D(64, (3, 3), kernel_initializer='lecun_normal')(x)\n",
    "    #     model.add(Conv2D(64, (3, 3)))\n",
    "        x = Activation('selu')(x)\n",
    "    #     model.add(Activation('relu'))\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    #     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        x = AlphaDropout(0.25)(x)\n",
    "    #     model.add(Dropout(0.25))\n",
    "        x = Flatten()(x)\n",
    "    #     model.add(Flatten())\n",
    "        x = Dense(512, kernel_initializer='lecun_normal')(x)\n",
    "    #     model.add(Dense(512))\n",
    "        x = Activation('selu')(x)\n",
    "    #     model.add(Activation('relu'))\n",
    "        x = AlphaDropout(0.5)(x)\n",
    "    #     model.add(Dropout(0.5))\n",
    "        predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    #     model.add(Dense(num_classes))\n",
    "    #     model.add(Activation('softmax'))\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "        # initiate RMSprop optimizer\n",
    "        if opt == 'rmsp':\n",
    "            rmsp = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "        # Let's train the model using RMSprop\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=rmsp,\n",
    "                          metrics=['accuracy'])\n",
    "        else:\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=opt,\n",
    "                          metrics=['accuracy'])\n",
    "            \n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "        x_train /= 255\n",
    "        x_test /= 255\n",
    "\n",
    "        csv_logger = CSVLogger('../logs/cifar_10/orig_selu_' + opt +'_log_%d.csv' % experiment, append=True, separator=';')\n",
    "        tb = TensorBoard(log_dir='./tb_logs/' + model_name + str(experiment), histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "        model.fit(x_train, y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(x_test, y_test),\n",
    "                  shuffle=True,\n",
    "                 verbose=1, callbacks=[csv_logger, tb])\n",
    "\n",
    "        # Save model and weights\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        model_path = os.path.join(save_dir, model_name + str(experiment) + \".h5\")\n",
    "        model.save(model_path)\n",
    "        print('Saved trained model at %s ' % model_path)\n",
    "        # Score trained model.\n",
    "        scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "        print('Test loss:', scores[0])\n",
    "        print('Test accuracy:', scores[1])\n",
    "\n",
    "        metrics['orig_selu_' + opt]['val_loss'].append(scores[0])\n",
    "        metrics['orig_selu_' + opt]['val_acc'].append(scores[1])\n",
    "\n",
    "        metrics['orig_selu_' + opt]['avg_val_loss'] = sum(metrics['orig_selu_' + opt]['val_loss']) / len(metrics['orig_selu_' + opt]['val_loss'])\n",
    "        metrics['orig_selu_' + opt]['avg_val_acc'] = sum(metrics['orig_selu_' + opt]['val_acc']) / len(metrics['orig_selu_' + opt]['val_acc'])\n",
    "\n",
    "        print('Average Test loss after %d experiments: %f' % (experiment+1, metrics['orig_selu_' + opt]['avg_val_loss']))\n",
    "        print('Average Test accuracy after %d experiments: %f' % (experiment+1, metrics['orig_selu_' + opt]['avg_val_acc'] ))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train with randomized activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "activations = ['sigmoid', 'tanh', 'relu', 'linear', 'elu', 'selu']\n",
    "activations_sigmoid = ['sigmoid', 'tanh']\n",
    "activations_linear = ['relu', 'linear', 'elu', 'selu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using full activations set\n",
    "### Random activations applied only to the last Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*30)\n",
    "print('Randomized all activations on last dense layer')\n",
    "model_name = 'cifar10_rand_all_last_dense_'\n",
    "dense_units = 512\n",
    "\n",
    "for experiment in range(experiments):\n",
    "    keras.backend.clear_session()\n",
    "    print('-'*30)\n",
    "    print('Experiment', experiment+1)\n",
    "    \n",
    "    inputs = Input(shape=(32,32,3))\n",
    "    x = Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:])(inputs)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(32, (3, 3))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, (3, 3))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    dense_layer = []\n",
    "    for i in range(dense_units):\n",
    "        activation = activations[randint(0, len(activations)-1)]\n",
    "        dense_layer.append(Dense(1, activation=activation)(x))\n",
    "\n",
    "    x = keras.layers.concatenate(dense_layer)  \n",
    "\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    csv_logger = CSVLogger('../logs/cifar_10/rand_all_last_dense_log_%d.csv' % experiment, append=True, separator=';')\n",
    "    tb = TensorBoard(log_dir='./tb_logs/' + model_name + str(experiment), histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "             verbose=1, callbacks=[csv_logger, tb])\n",
    "\n",
    "\n",
    "    # Save model and weights\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_name + str(experiment) + \".h5\")\n",
    "    model.save(model_path)\n",
    "    print('Saved trained model at %s ' % model_path)\n",
    "    scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    \n",
    "    metrics['rand_all_last_dense']['val_loss'].append(scores[0])\n",
    "    metrics['rand_all_last_dense']['val_acc'].append(scores[1])\n",
    "    \n",
    "    metrics['rand_all_last_dense']['avg_val_loss'] = sum(metrics['rand_all_last_dense']['val_loss']) / len(metrics['rand_all_last_dense']['val_loss'])\n",
    "    metrics['rand_all_last_dense']['avg_val_acc'] = sum(metrics['rand_all_last_dense']['val_acc']) / len(metrics['rand_all_last_dense']['val_acc'])\n",
    "    \n",
    "    print('Average Test loss after %d experiments: %f' % (experiment+1, metrics['rand_all_last_dense']['avg_val_loss']))\n",
    "    print('Average Test accuracy after %d experiments: %f' % (experiment+1, metrics['rand_all_last_dense']['avg_val_acc'] ))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random activations applied to all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*30)\n",
    "print('Randomized all activations on all layers')\n",
    "model_name = 'cifar10_rand_all_all_'\n",
    "dense_units = 512\n",
    "\n",
    "for experiment in range(experiments):\n",
    "    keras.backend.clear_session()\n",
    "    print('-'*30)\n",
    "    print('Experiment', experiment+1)\n",
    "    \n",
    "    inputs = Input(shape=(32,32,3))\n",
    "    \n",
    "#     x = Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:])(inputs)\n",
    "    layer = []\n",
    "    for i in range(32):\n",
    "        activation = activations[randint(0, len(activations)-1)]\n",
    "        layer.append(Conv2D(1, (3, 3), padding='same', activation=activation, input_shape=x_train.shape[1:])(inputs))\n",
    "\n",
    "    x = keras.layers.concatenate(layer)\n",
    "\n",
    "#     x = Conv2D(32, (3, 3))(x)\n",
    "    layer = []\n",
    "    for i in range(32):\n",
    "        activation = activations[randint(0, len(activations)-1)]\n",
    "        layer.append(Conv2D(1, (3, 3), activation=activation)(x))\n",
    "\n",
    "    x = keras.layers.concatenate(layer)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "#     x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    layer = []\n",
    "    for i in range(64):\n",
    "        activation = activations[randint(0, len(activations)-1)]\n",
    "        layer.append(Conv2D(1, (3, 3), activation=activation)(x))\n",
    "\n",
    "    x = keras.layers.concatenate(layer)\n",
    "\n",
    "#     x = Conv2D(64, (3, 3))(x)\n",
    "    layer = []\n",
    "    for i in range(64):\n",
    "        activation = activations[randint(0, len(activations)-1)]\n",
    "        layer.append(Conv2D(1, (3, 3), activation=activation)(x))\n",
    "\n",
    "    x = keras.layers.concatenate(layer)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    layer = []\n",
    "    for i in range(dense_units):\n",
    "        activation = activations[randint(0, len(activations)-1)]\n",
    "        layer.append(Dense(1, activation=activation)(x))\n",
    "\n",
    "    x = keras.layers.concatenate(layer)  \n",
    "\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    csv_logger = CSVLogger('../logs/cifar_10/rand_all_all_log_%d.csv' % experiment, append=True, separator=';')\n",
    "    tb = TensorBoard(log_dir='./tb_logs/' + model_name + str(experiment), histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "             verbose=1, callbacks=[csv_logger, tb])\n",
    "\n",
    "\n",
    "    # Save model and weights\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_name + str(experiment) + \".h5\")\n",
    "    model.save(model_path)\n",
    "    print('Saved trained model at %s ' % model_path)\n",
    "    scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    \n",
    "    metrics['rand_all_all']['val_loss'].append(scores[0])\n",
    "    metrics['rand_all_all']['val_acc'].append(scores[1])\n",
    "    \n",
    "    metrics['rand_all_all']['avg_val_loss'] = sum(metrics['rand_all_all']['val_loss']) / len(metrics['rand_all_all']['val_loss'])\n",
    "    metrics['rand_all_all']['avg_val_acc'] = sum(metrics['rand_all_all']['val_acc']) / len(metrics['rand_all_all']['val_acc'])\n",
    "    \n",
    "    print('Average Test loss after %d experiments: %f' % (experiment+1, metrics['rand_all_all']['avg_val_loss']))\n",
    "    print('Average Test accuracy after %d experiments: %f' % (experiment+1, metrics['rand_all_all']['avg_val_acc'] ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sigmoid activations set\n",
    "### Random activations applied only to the last Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*30)\n",
    "print('Randomized sigmoid activations on last dense layer')\n",
    "model_name = 'cifar10_rand_sig_last_dense_'\n",
    "dense_units = 512\n",
    "\n",
    "for experiment in range(experiments):\n",
    "    keras.backend.clear_session()\n",
    "    print('-'*30)\n",
    "    print('Experiment', experiment+1)\n",
    "    \n",
    "    inputs = Input(shape=(32,32,3))\n",
    "    x = Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:])(inputs)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(32, (3, 3))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, (3, 3))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    dense_layer = []\n",
    "    for i in range(dense_units):\n",
    "        activation = activations_sigmoid[randint(0, len(activations_sigmoid)-1)]\n",
    "        dense_layer.append(Dense(1, activation=activation)(x))\n",
    "\n",
    "    x = keras.layers.concatenate(dense_layer)  \n",
    "\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    csv_logger = CSVLogger('../logs/cifar_10/rand_sig_last_dense_log_%d.csv' % experiment, append=True, separator=';')\n",
    "    tb = TensorBoard(log_dir='./tb_logs/' + model_name + str(experiment), histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "             verbose=1, callbacks=[csv_logger, tb])\n",
    "\n",
    "\n",
    "    # Save model and weights\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_name + str(experiment) + \".h5\")\n",
    "    model.save(model_path)\n",
    "    print('Saved trained model at %s ' % model_path)\n",
    "    \n",
    "    scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    \n",
    "    metrics['rand_sig_last_dense']['val_loss'].append(scores[0])\n",
    "    metrics['rand_sig_last_dense']['val_acc'].append(scores[1])\n",
    "    \n",
    "    metrics['rand_sig_last_dense']['avg_val_loss'] = sum(metrics['rand_sig_last_dense']['val_loss']) / len(metrics['rand_sig_last_dense']['val_loss'])\n",
    "    metrics['rand_sig_last_dense']['avg_val_acc'] = sum(metrics['rand_sig_last_dense']['val_acc']) / len(metrics['rand_sig_last_dense']['val_acc'])\n",
    "    \n",
    "    print('Average Test loss after %d experiments: %f' % (experiment+1, metrics['rand_sig_last_dense']['avg_val_loss']))\n",
    "    print('Average Test accuracy after %d experiments: %f' % (experiment+1, metrics['rand_sig_last_dense']['avg_val_acc'] ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random activations applied to all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*30)\n",
    "print('Randomized sigmoid activations on all layers')\n",
    "model_name = 'cifar10_rand_sig_all_'\n",
    "dense_units = 512\n",
    "\n",
    "for experiment in range(experiments):\n",
    "    keras.backend.clear_session()\n",
    "    print('-'*30)\n",
    "    print('Experiment', experiment+1)\n",
    "    \n",
    "    inputs = Input(shape=(32,32,3))\n",
    "    \n",
    "#     x = Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:])(inputs)\n",
    "    layer = []\n",
    "    for i in range(32):\n",
    "        activation = activations_sigmoid[randint(0, len(activations_sigmoid)-1)]\n",
    "        layer.append(Conv2D(1, (3, 3), padding='same', activation=activation, input_shape=x_train.shape[1:])(inputs))\n",
    "\n",
    "    x = keras.layers.concatenate(layer)\n",
    "\n",
    "#     x = Conv2D(32, (3, 3))(x)\n",
    "    layer = []\n",
    "    for i in range(32):\n",
    "        activation = activations_sigmoid[randint(0, len(activations_sigmoid)-1)]\n",
    "        layer.append(Conv2D(1, (3, 3), activation=activation)(x))\n",
    "\n",
    "    x = keras.layers.concatenate(layer)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "#     x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    layer = []\n",
    "    for i in range(64):\n",
    "        activation = activations_sigmoid[randint(0, len(activations_sigmoid)-1)]\n",
    "        layer.append(Conv2D(1, (3, 3), activation=activation)(x))\n",
    "\n",
    "    x = keras.layers.concatenate(layer)\n",
    "\n",
    "#     x = Conv2D(64, (3, 3))(x)\n",
    "    layer = []\n",
    "    for i in range(64):\n",
    "        activation = activations_sigmoid[randint(0, len(activations_sigmoid)-1)]\n",
    "        layer.append(Conv2D(1, (3, 3), activation=activation)(x))\n",
    "\n",
    "    x = keras.layers.concatenate(layer)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    layer = []\n",
    "    for i in range(dense_units):\n",
    "        activation = activations_sigmoid[randint(0, len(activations_sigmoid)-1)]\n",
    "        layer.append(Dense(1, activation=activation)(x))\n",
    "\n",
    "    x = keras.layers.concatenate(layer)  \n",
    "\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    csv_logger = CSVLogger('../logs/cifar_10/rand_sig_all_log_%d.csv' % experiment, append=True, separator=';')\n",
    "    tb = TensorBoard(log_dir='./tb_logs/' + model_name + str(experiment), histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "             verbose=1, callbacks=[csv_logger, tb])\n",
    "\n",
    "\n",
    "    # Save model and weights\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_name + str(experiment) + \".h5\")\n",
    "    model.save(model_path)\n",
    "    print('Saved trained model at %s ' % model_path)\n",
    "    scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    \n",
    "    metrics['rand_sig_all']['val_loss'].append(scores[0])\n",
    "    metrics['rand_sig_all']['val_acc'].append(scores[1])\n",
    "    \n",
    "    metrics['rand_sig_all']['avg_val_loss'] = sum(metrics['rand_sig_all']['val_loss']) / len(metrics['rand_sig_all']['val_loss'])\n",
    "    metrics['rand_sig_all']['avg_val_acc'] = sum(metrics['rand_sig_all']['val_acc']) / len(metrics['rand_sig_all']['val_acc'])\n",
    "    \n",
    "    print('Average Test loss after %d experiments: %f' % (experiment+1, metrics['rand_sig_all']['avg_val_loss']))\n",
    "    print('Average Test accuracy after %d experiments: %f' % (experiment+1, metrics['rand_sig_all']['avg_val_acc'] ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using linear activations set\n",
    "### Random activations applied only to the last Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*30)\n",
    "print('Randomized linear activations on last dense layer')\n",
    "model_name = 'cifar10_rand_lin_last_dense_'\n",
    "dense_units = 512\n",
    "\n",
    "for experiment in range(experiments):\n",
    "    keras.backend.clear_session()\n",
    "    print('-'*30)\n",
    "    print('Experiment', experiment+1)\n",
    "    \n",
    "    inputs = Input(shape=(32,32,3))\n",
    "    x = Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:])(inputs)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(32, (3, 3))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, (3, 3))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    dense_layer = []\n",
    "    for i in range(dense_units):\n",
    "        activation = activations_linear[randint(0, len(activations_linear)-1)]\n",
    "        dense_layer.append(Dense(1, activation=activation)(x))\n",
    "\n",
    "    x = keras.layers.concatenate(dense_layer)  \n",
    "\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    csv_logger = CSVLogger('../logs/cifar_10/rand_lin_last_dense_log_%d.csv' % experiment, append=True, separator=';')\n",
    "    tb = TensorBoard(log_dir='./tb_logs/' + model_name + str(experiment), histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "             verbose=1, callbacks=[csv_logger, tb])\n",
    "\n",
    "\n",
    "    # Save model and weights\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_name + str(experiment) + \".h5\")\n",
    "    model.save(model_path)\n",
    "    print('Saved trained model at %s ' % model_path)\n",
    "    \n",
    "    scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    \n",
    "    metrics['rand_lin_last_dense']['val_loss'].append(scores[0])\n",
    "    metrics['rand_lin_last_dense']['val_acc'].append(scores[1])\n",
    "    \n",
    "    metrics['rand_lin_last_dense']['avg_val_loss'] = sum(metrics['rand_lin_last_dense']['val_loss']) / len(metrics['rand_lin_last_dense']['val_loss'])\n",
    "    metrics['rand_lin_last_dense']['avg_val_acc'] = sum(metrics['rand_lin_last_dense']['val_acc']) / len(metrics['rand_lin_last_dense']['val_acc'])\n",
    "    \n",
    "    print('Average Test loss after %d experiments: %f' % (experiment+1, metrics['rand_lin_last_dense']['avg_val_loss']))\n",
    "    print('Average Test accuracy after %d experiments: %f' % (experiment+1, metrics['rand_lin_last_dense']['avg_val_acc'] ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random activations applied to all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*30)\n",
    "print('Randomized linear activations on all layers')\n",
    "model_name = 'cifar10_rand_lin_all_'\n",
    "dense_units = 512\n",
    "\n",
    "for experiment in range(experiments):\n",
    "    keras.backend.clear_session()\n",
    "    print('-'*30)\n",
    "    print('Experiment', experiment+1)\n",
    "    \n",
    "    inputs = Input(shape=(32,32,3))\n",
    "    \n",
    "#     x = Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:])(inputs)\n",
    "    layer = []\n",
    "    for i in range(32):\n",
    "        activation = activations_linear[randint(0, len(activations_linear)-1)]\n",
    "        layer.append(Conv2D(1, (3, 3), padding='same', activation=activation, input_shape=x_train.shape[1:])(inputs))\n",
    "\n",
    "    x = keras.layers.concatenate(layer)\n",
    "\n",
    "#     x = Conv2D(32, (3, 3))(x)\n",
    "    layer = []\n",
    "    for i in range(32):\n",
    "        activation = activations_linear[randint(0, len(activations_linear)-1)]\n",
    "        layer.append(Conv2D(1, (3, 3), activation=activation)(x))\n",
    "\n",
    "    x = keras.layers.concatenate(layer)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "#     x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    layer = []\n",
    "    for i in range(64):\n",
    "        activation = activations_linear[randint(0, len(activations_linear)-1)]\n",
    "        layer.append(Conv2D(1, (3, 3), activation=activation)(x))\n",
    "\n",
    "    x = keras.layers.concatenate(layer)\n",
    "\n",
    "#     x = Conv2D(64, (3, 3))(x)\n",
    "    layer = []\n",
    "    for i in range(64):\n",
    "        activation = activations_linear[randint(0, len(activations_linear)-1)]\n",
    "        layer.append(Conv2D(1, (3, 3), activation=activation)(x))\n",
    "\n",
    "    x = keras.layers.concatenate(layer)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    layer = []\n",
    "    for i in range(dense_units):\n",
    "        activation = activations_linear[randint(0, len(activations_linear)-1)]\n",
    "        layer.append(Dense(1, activation=activation)(x))\n",
    "\n",
    "    x = keras.layers.concatenate(layer)  \n",
    "\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    csv_logger = CSVLogger('../logs/cifar_10/rand_lin_all_log_%d.csv' % experiment, append=True, separator=';')\n",
    "    tb = TensorBoard(log_dir='./tb_logs/' + model_name + str(experiment), histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "             verbose=1, callbacks=[csv_logger, tb])\n",
    "\n",
    "\n",
    "    # Save model and weights\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_name + str(experiment) + \".h5\")\n",
    "    model.save(model_path)\n",
    "    print('Saved trained model at %s ' % model_path)\n",
    "    scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    \n",
    "    metrics['rand_lin_all']['val_loss'].append(scores[0])\n",
    "    metrics['rand_lin_all']['val_acc'].append(scores[1])\n",
    "    \n",
    "    metrics['rand_lin_all']['avg_val_loss'] = sum(metrics['rand_lin_all']['val_loss']) / len(metrics['rand_lin_all']['val_loss'])\n",
    "    metrics['rand_lin_all']['avg_val_acc'] = sum(metrics['rand_lin_all']['val_acc']) / len(metrics['rand_lin_all']['val_acc'])\n",
    "    \n",
    "    print('Average Test loss after %d experiments: %f' % (experiment+1, metrics['rand_lin_all']['avg_val_loss']))\n",
    "    print('Average Test accuracy after %d experiments: %f' % (experiment+1, metrics['rand_lin_all']['avg_val_acc'] ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Comparing Test Loss:')\n",
    "for act in activations_orig:\n",
    "    print('Original ' + act + ':', metrics['orig_'+ act]['avg_val_loss'])\n",
    "for opt in opts:\n",
    "    print('Original SeLU ' + opt + ':', metrics['orig_selu_' + opt]['avg_val_loss'])\n",
    "print('Random All activations, All layers:', metrics['rand_all_all']['avg_val_loss'])\n",
    "print('Random All activations, Last dense:', metrics['rand_all_last_dense']['avg_val_loss'])\n",
    "print('Random Sigmoid activations, All layers:', metrics['rand_sig_all']['avg_val_loss'])\n",
    "print('Random Sigmoid activations, Last dense:', metrics['rand_sig_last_dense']['avg_val_loss'])\n",
    "print('Random Linear activations, All layers:', metrics['rand_lin_all']['avg_val_loss'])\n",
    "print('Random Linear activations, Last dense:', metrics['rand_lin_last_dense']['avg_val_loss'])\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "print('Comparing Test Accuracy:')\n",
    "for act in activations_orig:\n",
    "    print('Original ' + act + ':', metrics['orig_'+ act]['avg_val_acc'])\n",
    "for opt in opts:\n",
    "    print('Original SeLU ' + opt + ':', metrics['orig_selu_' + opt]['avg_val_acc'])\n",
    "print('Random All activations, All layers:', metrics['rand_all_all']['avg_val_acc'])\n",
    "print('Random All activations, Last dense:', metrics['rand_all_last_dense']['avg_val_acc'])\n",
    "print('Random Sigmoid activations, All layers:', metrics['rand_sig_all']['avg_val_acc'])\n",
    "print('Random Sigmoid activations, Last dense:', metrics['rand_sig_last_dense']['avg_val_acc'])\n",
    "print('Random Linear activations, All layers:', metrics['rand_lin_all']['avg_val_acc'])\n",
    "print('Random Linear activations, Last dense:', metrics['rand_lin_last_dense']['avg_val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
