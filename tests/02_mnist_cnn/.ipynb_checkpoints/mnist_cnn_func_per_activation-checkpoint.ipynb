{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trains a simple deep NN on the MNIST dataset.\n",
    "Gets to 98.40% test accuracy after 20 epochs\n",
    "(there is *a lot* of margin for parameter tuning).\n",
    "2 seconds per epoch on a K520 GPU.\n",
    "'''\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import CSVLogger, TensorBoard, ReduceLROnPlateau, EarlyStopping\n",
    "import os\n",
    "from keras.layers.noise import AlphaDropout\n",
    "import keras.activations\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 40\n",
    "units = 64\n",
    "experiments = 1\n",
    "start = 0\n",
    "add_final_dense = False\n",
    "# activations = ['sigmoid', 'tanh', 'relu', 'linear', 'elu', 'softplus', 'softsign', 'hard_sigmoid', 'LeakyReLU', 'ThresholdedReLU']\n",
    "activations = ['tanh', 'relu', 'linear', 'softplus', 'softsign', 'hard_sigmoid', 'LeakyReLU', 'ThresholdedReLU']\n",
    "# PReLU is not used, since it does not currently support variable inputs\n",
    "optimizers = ['rmsp', 'adam', 'sgd', 'Adagrad', 'Adadelta', 'Adamax', 'Nadam']\n",
    "# optimizers = ['Nadam']\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "start_time = time.time()\n",
    "counter = 0\n",
    "total_items = (len(activations) + 1 )* len(optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train per each activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for activation tanh with optimizer rmsp\n",
      "------------------------------\n",
      "Experiment 0\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 254s 4ms/step - loss: 0.1814 - acc: 0.9459 - val_loss: 0.0682 - val_acc: 0.9780\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 263s 4ms/step - loss: 0.0798 - acc: 0.9764 - val_loss: 0.0652 - val_acc: 0.9793\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 252s 4ms/step - loss: 0.0638 - acc: 0.9813 - val_loss: 0.0724 - val_acc: 0.9794\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 261s 4ms/step - loss: 0.0560 - acc: 0.9829 - val_loss: 0.0763 - val_acc: 0.9772\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 258s 4ms/step - loss: 0.0504 - acc: 0.9850 - val_loss: 0.0565 - val_acc: 0.9834\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 255s 4ms/step - loss: 0.0436 - acc: 0.9876 - val_loss: 0.0621 - val_acc: 0.9832\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 258s 4ms/step - loss: 0.0367 - acc: 0.9890 - val_loss: 0.0610 - val_acc: 0.9832\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 264s 4ms/step - loss: 0.0356 - acc: 0.9896 - val_loss: 0.0755 - val_acc: 0.9825\n",
      "Epoch 9/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9908\n",
      "Epoch 00009: reducing learning rate to 0.00020000000949949026.\n",
      "60000/60000 [==============================] - 261s 4ms/step - loss: 0.0312 - acc: 0.9908 - val_loss: 0.0859 - val_acc: 0.9809\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 259s 4ms/step - loss: 0.0140 - acc: 0.9953 - val_loss: 0.0656 - val_acc: 0.9848\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 263s 4ms/step - loss: 0.0104 - acc: 0.9969 - val_loss: 0.0614 - val_acc: 0.9862\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 257s 4ms/step - loss: 0.0096 - acc: 0.9973 - val_loss: 0.0633 - val_acc: 0.9859\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 254s 4ms/step - loss: 0.0085 - acc: 0.9975 - val_loss: 0.0671 - val_acc: 0.9859\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 244s 4ms/step - loss: 0.0078 - acc: 0.9978 - val_loss: 0.0670 - val_acc: 0.9860\n",
      "Epoch 15/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9980\n",
      "Epoch 00015: reducing learning rate to 4.0000001899898055e-05.\n",
      "60000/60000 [==============================] - 280s 5ms/step - loss: 0.0073 - acc: 0.9980 - val_loss: 0.0698 - val_acc: 0.9854\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 219s 4ms/step - loss: 0.0057 - acc: 0.9983 - val_loss: 0.0687 - val_acc: 0.9856\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 242s 4ms/step - loss: 0.0055 - acc: 0.9984 - val_loss: 0.0681 - val_acc: 0.9852\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 277s 5ms/step - loss: 0.0054 - acc: 0.9986 - val_loss: 0.0674 - val_acc: 0.9853\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 261s 4ms/step - loss: 0.0052 - acc: 0.9986 - val_loss: 0.0669 - val_acc: 0.9855\n",
      "Epoch 20/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9984\n",
      "Epoch 00020: reducing learning rate to 8.000000525498762e-06.\n",
      "60000/60000 [==============================] - 255s 4ms/step - loss: 0.0051 - acc: 0.9985 - val_loss: 0.0681 - val_acc: 0.9854\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 237s 4ms/step - loss: 0.0051 - acc: 0.9987 - val_loss: 0.0678 - val_acc: 0.9855\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 237s 4ms/step - loss: 0.0044 - acc: 0.9989 - val_loss: 0.0678 - val_acc: 0.9856\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 256s 4ms/step - loss: 0.0050 - acc: 0.9985 - val_loss: 0.0676 - val_acc: 0.9856\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 258s 4ms/step - loss: 0.0048 - acc: 0.9988 - val_loss: 0.0675 - val_acc: 0.9857\n",
      "Epoch 25/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9987\n",
      "Epoch 00025: reducing learning rate to 1.6000001778593287e-06.\n",
      "60000/60000 [==============================] - 232s 4ms/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.0679 - val_acc: 0.9859\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 242s 4ms/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.0678 - val_acc: 0.9859\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 250s 4ms/step - loss: 0.0046 - acc: 0.9989 - val_loss: 0.0678 - val_acc: 0.9858\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 263s 4ms/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.0677 - val_acc: 0.9857\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 278s 5ms/step - loss: 0.0043 - acc: 0.9990 - val_loss: 0.0678 - val_acc: 0.9858\n",
      "Epoch 30/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9989\n",
      "Epoch 00030: reducing learning rate to 3.200000264769187e-07.\n",
      "60000/60000 [==============================] - 254s 4ms/step - loss: 0.0044 - acc: 0.9989 - val_loss: 0.0677 - val_acc: 0.9858\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 262s 4ms/step - loss: 0.0047 - acc: 0.9986 - val_loss: 0.0677 - val_acc: 0.9858\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 256s 4ms/step - loss: 0.0048 - acc: 0.9988 - val_loss: 0.0677 - val_acc: 0.9858\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 258s 4ms/step - loss: 0.0042 - acc: 0.9990 - val_loss: 0.0677 - val_acc: 0.9858\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 257s 4ms/step - loss: 0.0049 - acc: 0.9987 - val_loss: 0.0677 - val_acc: 0.9858\n",
      "Epoch 35/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9988\n",
      "Epoch 00035: reducing learning rate to 6.400000529538374e-08.\n",
      "60000/60000 [==============================] - 229s 4ms/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.0677 - val_acc: 0.9858\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.0045 - acc: 0.9987 - val_loss: 0.0677 - val_acc: 0.9858\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 260s 4ms/step - loss: 0.0043 - acc: 0.9989 - val_loss: 0.0677 - val_acc: 0.9858\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 258s 4ms/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.0677 - val_acc: 0.9858\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 260s 4ms/step - loss: 0.0048 - acc: 0.9987 - val_loss: 0.0677 - val_acc: 0.9858\n",
      "Epoch 40/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9991\n",
      "Epoch 00040: reducing learning rate to 1.2800001059076749e-08.\n",
      "60000/60000 [==============================] - 260s 4ms/step - loss: 0.0041 - acc: 0.9991 - val_loss: 0.0677 - val_acc: 0.9858\n",
      "Test loss: 0.0676854922968\n",
      "Test accuracy: 0.9858\n",
      "Remaining time: 7 days 7 hours 07 minutes 08 seconds\n",
      "\n",
      "Training for activation tanh with optimizer adam\n",
      "------------------------------\n",
      "Experiment 0\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 256s 4ms/step - loss: 0.2026 - acc: 0.9399 - val_loss: 0.0882 - val_acc: 0.9710\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 244s 4ms/step - loss: 0.0816 - acc: 0.9761 - val_loss: 0.0756 - val_acc: 0.9761\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 233s 4ms/step - loss: 0.0657 - acc: 0.9801 - val_loss: 0.0654 - val_acc: 0.9800\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 243s 4ms/step - loss: 0.0567 - acc: 0.9831 - val_loss: 0.0947 - val_acc: 0.9738\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 277s 5ms/step - loss: 0.0494 - acc: 0.9844 - val_loss: 0.0667 - val_acc: 0.9815\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 259s 4ms/step - loss: 0.0480 - acc: 0.9848 - val_loss: 0.0735 - val_acc: 0.9804\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 245s 4ms/step - loss: 0.0410 - acc: 0.9869 - val_loss: 0.0854 - val_acc: 0.9798\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 243s 4ms/step - loss: 0.0380 - acc: 0.9882 - val_loss: 0.0744 - val_acc: 0.9817\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 265s 4ms/step - loss: 0.0342 - acc: 0.9892 - val_loss: 0.0771 - val_acc: 0.9817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 238s 4ms/step - loss: 0.0340 - acc: 0.9891 - val_loss: 0.0919 - val_acc: 0.9805\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 271s 5ms/step - loss: 0.0312 - acc: 0.9903 - val_loss: 0.0892 - val_acc: 0.9822\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 238s 4ms/step - loss: 0.0282 - acc: 0.9912 - val_loss: 0.0996 - val_acc: 0.9807\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 256s 4ms/step - loss: 0.0278 - acc: 0.9916 - val_loss: 0.1042 - val_acc: 0.9792\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 238s 4ms/step - loss: 0.0260 - acc: 0.9916 - val_loss: 0.1033 - val_acc: 0.9818\n",
      "Epoch 15/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9926\n",
      "Epoch 00015: reducing learning rate to 0.00020000000949949026.\n",
      "60000/60000 [==============================] - 233s 4ms/step - loss: 0.0245 - acc: 0.9926 - val_loss: 0.1251 - val_acc: 0.9779\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 231s 4ms/step - loss: 0.0109 - acc: 0.9961 - val_loss: 0.0916 - val_acc: 0.9832\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 232s 4ms/step - loss: 0.0064 - acc: 0.9976 - val_loss: 0.0909 - val_acc: 0.9839\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 241s 4ms/step - loss: 0.0057 - acc: 0.9980 - val_loss: 0.0918 - val_acc: 0.9832\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 249s 4ms/step - loss: 0.0050 - acc: 0.9983 - val_loss: 0.0914 - val_acc: 0.9827\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 251s 4ms/step - loss: 0.0045 - acc: 0.9984 - val_loss: 0.0899 - val_acc: 0.9832\n",
      "Epoch 21/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9983\n",
      "Epoch 00021: reducing learning rate to 4.0000001899898055e-05.\n",
      "60000/60000 [==============================] - 262s 4ms/step - loss: 0.0051 - acc: 0.9984 - val_loss: 0.0909 - val_acc: 0.9838\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 213s 4ms/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0909 - val_acc: 0.9834\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 211s 4ms/step - loss: 0.0031 - acc: 0.9989 - val_loss: 0.0912 - val_acc: 0.9833\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 220s 4ms/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0910 - val_acc: 0.9835\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0919 - val_acc: 0.9834\n",
      "Epoch 26/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9993\n",
      "Epoch 00026: reducing learning rate to 8.000000525498762e-06.\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0904 - val_acc: 0.9834\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.0907 - val_acc: 0.9832\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0909 - val_acc: 0.9833\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 206s 3ms/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0904 - val_acc: 0.9832\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0902 - val_acc: 0.9836\n",
      "Epoch 31/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9993\n",
      "Epoch 00031: reducing learning rate to 1.6000001778593287e-06.\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0905 - val_acc: 0.9838\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0905 - val_acc: 0.9836\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 211s 4ms/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0904 - val_acc: 0.9836\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 223s 4ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0904 - val_acc: 0.9836\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 247s 4ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0903 - val_acc: 0.9839\n",
      "Epoch 36/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 00036: reducing learning rate to 3.200000264769187e-07.\n",
      "60000/60000 [==============================] - 239s 4ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0903 - val_acc: 0.9838\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 227s 4ms/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0903 - val_acc: 0.9836\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 216s 4ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0903 - val_acc: 0.9836\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0903 - val_acc: 0.9837\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0902 - val_acc: 0.9837\n",
      "Test loss: 0.0902440554044\n",
      "Test accuracy: 0.9837\n",
      "Remaining time: 6 days 20 hours 41 minutes 01 seconds\n",
      "\n",
      "Training for activation tanh with optimizer sgd\n",
      "------------------------------\n",
      "Experiment 0\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.5716 - acc: 0.8482 - val_loss: 0.2919 - val_acc: 0.9160\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.2933 - acc: 0.9144 - val_loss: 0.2565 - val_acc: 0.9237\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.2525 - acc: 0.9259 - val_loss: 0.2236 - val_acc: 0.9348\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.2212 - acc: 0.9362 - val_loss: 0.1962 - val_acc: 0.9457\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.1942 - acc: 0.9441 - val_loss: 0.1660 - val_acc: 0.9533\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.1702 - acc: 0.9509 - val_loss: 0.1511 - val_acc: 0.9590\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.1506 - acc: 0.9563 - val_loss: 0.1310 - val_acc: 0.9629\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 202s 3ms/step - loss: 0.1347 - acc: 0.9620 - val_loss: 0.1175 - val_acc: 0.9666\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.1209 - acc: 0.9659 - val_loss: 0.1036 - val_acc: 0.9710\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.1106 - acc: 0.9684 - val_loss: 0.0968 - val_acc: 0.9720\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 0.1015 - acc: 0.9705 - val_loss: 0.0899 - val_acc: 0.9741\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.0943 - acc: 0.9732 - val_loss: 0.0830 - val_acc: 0.9757\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0890 - acc: 0.9746 - val_loss: 0.0780 - val_acc: 0.9766\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 206s 3ms/step - loss: 0.0838 - acc: 0.9760 - val_loss: 0.0762 - val_acc: 0.9775\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0792 - acc: 0.9776 - val_loss: 0.0707 - val_acc: 0.9780\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0749 - acc: 0.9788 - val_loss: 0.0674 - val_acc: 0.9790\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.0721 - acc: 0.9792 - val_loss: 0.0673 - val_acc: 0.9796\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 0.0682 - acc: 0.9802 - val_loss: 0.0629 - val_acc: 0.9796\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 217s 4ms/step - loss: 0.0662 - acc: 0.9805 - val_loss: 0.0601 - val_acc: 0.9815\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0634 - acc: 0.9822 - val_loss: 0.0604 - val_acc: 0.9806\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.0613 - acc: 0.9827 - val_loss: 0.0566 - val_acc: 0.9827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0598 - acc: 0.9827 - val_loss: 0.0564 - val_acc: 0.9823\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0572 - acc: 0.9833 - val_loss: 0.0540 - val_acc: 0.9827\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0561 - acc: 0.9834 - val_loss: 0.0542 - val_acc: 0.9823\n",
      "Epoch 25/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9840\n",
      "Epoch 00025: reducing learning rate to 0.0019999999552965165.\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 0.0546 - acc: 0.9840 - val_loss: 0.0539 - val_acc: 0.9826\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0509 - acc: 0.9851 - val_loss: 0.0527 - val_acc: 0.9832\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 202s 3ms/step - loss: 0.0501 - acc: 0.9860 - val_loss: 0.0519 - val_acc: 0.9840\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 211s 4ms/step - loss: 0.0506 - acc: 0.9852 - val_loss: 0.0511 - val_acc: 0.9830\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0504 - acc: 0.9855 - val_loss: 0.0512 - val_acc: 0.9837\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 0.0498 - acc: 0.9857 - val_loss: 0.0515 - val_acc: 0.9830\n",
      "Epoch 31/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9854\n",
      "Epoch 00031: reducing learning rate to 0.0003999999724328518.\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.0496 - acc: 0.9854 - val_loss: 0.0503 - val_acc: 0.9833\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0491 - acc: 0.9862 - val_loss: 0.0506 - val_acc: 0.9836\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0487 - acc: 0.9861 - val_loss: 0.0507 - val_acc: 0.9841\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.0490 - acc: 0.9860 - val_loss: 0.0506 - val_acc: 0.9841\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0488 - acc: 0.9856 - val_loss: 0.0507 - val_acc: 0.9839\n",
      "Epoch 36/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9859\n",
      "Epoch 00036: reducing learning rate to 7.999999215826393e-05.\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0493 - acc: 0.9859 - val_loss: 0.0505 - val_acc: 0.9839\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.0486 - acc: 0.9861 - val_loss: 0.0505 - val_acc: 0.9835\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 206s 3ms/step - loss: 0.0489 - acc: 0.9861 - val_loss: 0.0505 - val_acc: 0.9835\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0490 - acc: 0.9857 - val_loss: 0.0505 - val_acc: 0.9837\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.0492 - acc: 0.9858 - val_loss: 0.0505 - val_acc: 0.9836\n",
      "Test loss: 0.050480847574\n",
      "Test accuracy: 0.9836\n",
      "Remaining time: 6 days 9 hours 37 minutes 28 seconds\n",
      "\n",
      "Training for activation tanh with optimizer Adagrad\n",
      "------------------------------\n",
      "Experiment 0\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.1870 - acc: 0.9459 - val_loss: 0.0875 - val_acc: 0.9737\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0817 - acc: 0.9761 - val_loss: 0.0627 - val_acc: 0.9798\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 202s 3ms/step - loss: 0.0639 - acc: 0.9812 - val_loss: 0.0582 - val_acc: 0.9814\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 0.0537 - acc: 0.9838 - val_loss: 0.0547 - val_acc: 0.9814\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0478 - acc: 0.9860 - val_loss: 0.0509 - val_acc: 0.9831\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0431 - acc: 0.9873 - val_loss: 0.0510 - val_acc: 0.9830\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0397 - acc: 0.9880 - val_loss: 0.0476 - val_acc: 0.9852\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 210s 3ms/step - loss: 0.0371 - acc: 0.9892 - val_loss: 0.0447 - val_acc: 0.9854\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 206s 3ms/step - loss: 0.0350 - acc: 0.9898 - val_loss: 0.0463 - val_acc: 0.9850\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0324 - acc: 0.9905 - val_loss: 0.0452 - val_acc: 0.9853\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 210s 3ms/step - loss: 0.0311 - acc: 0.9904 - val_loss: 0.0476 - val_acc: 0.9829\n",
      "Epoch 12/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9916\n",
      "Epoch 00012: reducing learning rate to 0.0019999999552965165.\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0292 - acc: 0.9916 - val_loss: 0.0454 - val_acc: 0.9852\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.0252 - acc: 0.9929 - val_loss: 0.0443 - val_acc: 0.9862\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 232s 4ms/step - loss: 0.0250 - acc: 0.9928 - val_loss: 0.0441 - val_acc: 0.9861\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 216s 4ms/step - loss: 0.0248 - acc: 0.9929 - val_loss: 0.0442 - val_acc: 0.9858\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 0.0240 - acc: 0.9931 - val_loss: 0.0442 - val_acc: 0.9859\n",
      "Epoch 17/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9934\n",
      "Epoch 00017: reducing learning rate to 0.0003999999724328518.\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 0.0241 - acc: 0.9934 - val_loss: 0.0438 - val_acc: 0.9862\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 211s 4ms/step - loss: 0.0236 - acc: 0.9934 - val_loss: 0.0439 - val_acc: 0.9860\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0234 - acc: 0.9938 - val_loss: 0.0441 - val_acc: 0.9863\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 202s 3ms/step - loss: 0.0233 - acc: 0.9935 - val_loss: 0.0440 - val_acc: 0.9862\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 210s 4ms/step - loss: 0.0233 - acc: 0.9933 - val_loss: 0.0439 - val_acc: 0.9862\n",
      "Epoch 22/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9935\n",
      "Epoch 00022: reducing learning rate to 7.999999215826393e-05.\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.0233 - acc: 0.9935 - val_loss: 0.0439 - val_acc: 0.9862\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0231 - acc: 0.9934 - val_loss: 0.0439 - val_acc: 0.9861\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 210s 3ms/step - loss: 0.0232 - acc: 0.9935 - val_loss: 0.0439 - val_acc: 0.9861\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0230 - acc: 0.9934 - val_loss: 0.0440 - val_acc: 0.9862\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0230 - acc: 0.9939 - val_loss: 0.0440 - val_acc: 0.9862\n",
      "Epoch 27/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9939\n",
      "Epoch 00027: reducing learning rate to 1.599999814061448e-05.\n",
      "60000/60000 [==============================] - 206s 3ms/step - loss: 0.0232 - acc: 0.9939 - val_loss: 0.0440 - val_acc: 0.9862\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0235 - acc: 0.9934 - val_loss: 0.0440 - val_acc: 0.9862\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0231 - acc: 0.9938 - val_loss: 0.0440 - val_acc: 0.9862\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 206s 3ms/step - loss: 0.0230 - acc: 0.9936 - val_loss: 0.0440 - val_acc: 0.9862\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0227 - acc: 0.9937 - val_loss: 0.0440 - val_acc: 0.9862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9933\n",
      "Epoch 00032: reducing learning rate to 3.199999628122896e-06.\n",
      "60000/60000 [==============================] - 202s 3ms/step - loss: 0.0237 - acc: 0.9933 - val_loss: 0.0440 - val_acc: 0.9862\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0221 - acc: 0.9941 - val_loss: 0.0440 - val_acc: 0.9862\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0230 - acc: 0.9938 - val_loss: 0.0440 - val_acc: 0.9862\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0233 - acc: 0.9938 - val_loss: 0.0440 - val_acc: 0.9862\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0232 - acc: 0.9935 - val_loss: 0.0440 - val_acc: 0.9862\n",
      "Epoch 37/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9936\n",
      "Epoch 00037: reducing learning rate to 6.399999165296323e-07.\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0234 - acc: 0.9936 - val_loss: 0.0440 - val_acc: 0.9862\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0235 - acc: 0.9935 - val_loss: 0.0440 - val_acc: 0.9862\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.0234 - acc: 0.9931 - val_loss: 0.0440 - val_acc: 0.9862\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0230 - acc: 0.9936 - val_loss: 0.0440 - val_acc: 0.9862\n",
      "Test loss: 0.0439745311859\n",
      "Test accuracy: 0.9862\n",
      "Remaining time: 6 days 3 hours 09 minutes 59 seconds\n",
      "\n",
      "Training for activation tanh with optimizer Adadelta\n",
      "------------------------------\n",
      "Experiment 0\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 211s 4ms/step - loss: 0.2844 - acc: 0.9175 - val_loss: 0.1315 - val_acc: 0.9634\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 211s 4ms/step - loss: 0.1108 - acc: 0.9680 - val_loss: 0.0752 - val_acc: 0.9772\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.0760 - acc: 0.9776 - val_loss: 0.0587 - val_acc: 0.9818\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 210s 4ms/step - loss: 0.0609 - acc: 0.9820 - val_loss: 0.0591 - val_acc: 0.9814\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 202s 3ms/step - loss: 0.0523 - acc: 0.9844 - val_loss: 0.0514 - val_acc: 0.9836\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0457 - acc: 0.9863 - val_loss: 0.0472 - val_acc: 0.9851\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 210s 3ms/step - loss: 0.0412 - acc: 0.9877 - val_loss: 0.0515 - val_acc: 0.9830\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 0.0367 - acc: 0.9889 - val_loss: 0.0428 - val_acc: 0.9855\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 206s 3ms/step - loss: 0.0335 - acc: 0.9896 - val_loss: 0.0495 - val_acc: 0.9833\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 222s 4ms/step - loss: 0.0311 - acc: 0.9904 - val_loss: 0.0434 - val_acc: 0.9865\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.0282 - acc: 0.9910 - val_loss: 0.0463 - val_acc: 0.9855\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.0255 - acc: 0.9922 - val_loss: 0.0490 - val_acc: 0.9861\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 210s 3ms/step - loss: 0.0238 - acc: 0.9927 - val_loss: 0.0477 - val_acc: 0.9852\n",
      "Epoch 14/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9936\n",
      "Epoch 00014: reducing learning rate to 0.2.\n",
      "60000/60000 [==============================] - 210s 4ms/step - loss: 0.0216 - acc: 0.9936 - val_loss: 0.0426 - val_acc: 0.9866\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0161 - acc: 0.9954 - val_loss: 0.0420 - val_acc: 0.9868\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0152 - acc: 0.9958 - val_loss: 0.0425 - val_acc: 0.9866\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 210s 4ms/step - loss: 0.0149 - acc: 0.9959 - val_loss: 0.0428 - val_acc: 0.9867\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0144 - acc: 0.9962 - val_loss: 0.0416 - val_acc: 0.9870\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0143 - acc: 0.9961 - val_loss: 0.0417 - val_acc: 0.9873\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 210s 4ms/step - loss: 0.0141 - acc: 0.9963 - val_loss: 0.0423 - val_acc: 0.9869\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0132 - acc: 0.9965 - val_loss: 0.0413 - val_acc: 0.9865\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0136 - acc: 0.9963 - val_loss: 0.0418 - val_acc: 0.9868\n",
      "Epoch 23/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9967\n",
      "Epoch 00023: reducing learning rate to 0.04000000059604645.\n",
      "60000/60000 [==============================] - 210s 3ms/step - loss: 0.0130 - acc: 0.9967 - val_loss: 0.0425 - val_acc: 0.9866\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 206s 3ms/step - loss: 0.0120 - acc: 0.9969 - val_loss: 0.0417 - val_acc: 0.9869\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.0117 - acc: 0.9970 - val_loss: 0.0416 - val_acc: 0.9870\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 210s 3ms/step - loss: 0.0120 - acc: 0.9969 - val_loss: 0.0417 - val_acc: 0.9868\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0121 - acc: 0.9970 - val_loss: 0.0416 - val_acc: 0.9870\n",
      "Epoch 28/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9971\n",
      "Epoch 00028: reducing learning rate to 0.007999999821186066.\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0118 - acc: 0.9971 - val_loss: 0.0416 - val_acc: 0.9869\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0119 - acc: 0.9972 - val_loss: 0.0416 - val_acc: 0.9868\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 210s 4ms/step - loss: 0.0121 - acc: 0.9967 - val_loss: 0.0416 - val_acc: 0.9868\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.0118 - acc: 0.9971 - val_loss: 0.0417 - val_acc: 0.9867\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 206s 3ms/step - loss: 0.0118 - acc: 0.9970 - val_loss: 0.0416 - val_acc: 0.9869\n",
      "Epoch 33/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9971\n",
      "Epoch 00033: reducing learning rate to 0.0015999998897314072.\n",
      "60000/60000 [==============================] - 210s 3ms/step - loss: 0.0116 - acc: 0.9971 - val_loss: 0.0416 - val_acc: 0.9869\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0115 - acc: 0.9973 - val_loss: 0.0416 - val_acc: 0.9869\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 0.0118 - acc: 0.9972 - val_loss: 0.0416 - val_acc: 0.9869\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 211s 4ms/step - loss: 0.0114 - acc: 0.9971 - val_loss: 0.0416 - val_acc: 0.9869\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0115 - acc: 0.9972 - val_loss: 0.0416 - val_acc: 0.9869\n",
      "Epoch 38/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9972\n",
      "Epoch 00038: reducing learning rate to 0.0003199999686330557.\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 0.0115 - acc: 0.9972 - val_loss: 0.0416 - val_acc: 0.9869\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 210s 3ms/step - loss: 0.0115 - acc: 0.9971 - val_loss: 0.0416 - val_acc: 0.9869\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 206s 3ms/step - loss: 0.0114 - acc: 0.9972 - val_loss: 0.0416 - val_acc: 0.9869\n",
      "Test loss: 0.0416243755002\n",
      "Test accuracy: 0.9869\n",
      "Remaining time: 5 days 22 hours 25 minutes 36 seconds\n",
      "\n",
      "Training for activation tanh with optimizer Adamax\n",
      "------------------------------\n",
      "Experiment 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.2143 - acc: 0.9376 - val_loss: 0.0819 - val_acc: 0.9742\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0816 - acc: 0.9757 - val_loss: 0.0674 - val_acc: 0.9779\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 210s 4ms/step - loss: 0.0614 - acc: 0.9812 - val_loss: 0.0575 - val_acc: 0.9814\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0510 - acc: 0.9838 - val_loss: 0.0550 - val_acc: 0.9817\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 219s 4ms/step - loss: 0.0437 - acc: 0.9861 - val_loss: 0.0550 - val_acc: 0.9829\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 214s 4ms/step - loss: 0.0373 - acc: 0.9880 - val_loss: 0.0478 - val_acc: 0.9840\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 0.0317 - acc: 0.9899 - val_loss: 0.0517 - val_acc: 0.9845\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0270 - acc: 0.9911 - val_loss: 0.0523 - val_acc: 0.9843\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 210s 4ms/step - loss: 0.0248 - acc: 0.9918 - val_loss: 0.0559 - val_acc: 0.9816\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 206s 3ms/step - loss: 0.0224 - acc: 0.9927 - val_loss: 0.0579 - val_acc: 0.9841\n",
      "Epoch 11/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9933\n",
      "Epoch 00011: reducing learning rate to 0.0004000000189989805.\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0202 - acc: 0.9933 - val_loss: 0.0574 - val_acc: 0.9845\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 211s 4ms/step - loss: 0.0117 - acc: 0.9962 - val_loss: 0.0487 - val_acc: 0.9864\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.0093 - acc: 0.9974 - val_loss: 0.0482 - val_acc: 0.9872\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0089 - acc: 0.9974 - val_loss: 0.0495 - val_acc: 0.9868\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0088 - acc: 0.9978 - val_loss: 0.0494 - val_acc: 0.9866\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0083 - acc: 0.9977 - val_loss: 0.0484 - val_acc: 0.9870\n",
      "Epoch 17/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9978\n",
      "Epoch 00017: reducing learning rate to 8.000000379979611e-05.\n",
      "60000/60000 [==============================] - 210s 4ms/step - loss: 0.0081 - acc: 0.9978 - val_loss: 0.0494 - val_acc: 0.9871\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 206s 3ms/step - loss: 0.0066 - acc: 0.9984 - val_loss: 0.0484 - val_acc: 0.9870\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 210s 3ms/step - loss: 0.0067 - acc: 0.9984 - val_loss: 0.0483 - val_acc: 0.9872\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 202s 3ms/step - loss: 0.0069 - acc: 0.9981 - val_loss: 0.0486 - val_acc: 0.9874\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0067 - acc: 0.9984 - val_loss: 0.0487 - val_acc: 0.9872\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0064 - acc: 0.9985 - val_loss: 0.0487 - val_acc: 0.9870\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0062 - acc: 0.9987 - val_loss: 0.0483 - val_acc: 0.9869\n",
      "Epoch 24/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9986\n",
      "Epoch 00024: reducing learning rate to 1.6000001050997525e-05.\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0065 - acc: 0.9986 - val_loss: 0.0490 - val_acc: 0.9867\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 210s 3ms/step - loss: 0.0062 - acc: 0.9986 - val_loss: 0.0488 - val_acc: 0.9868\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0065 - acc: 0.9984 - val_loss: 0.0487 - val_acc: 0.9868\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.0061 - acc: 0.9985 - val_loss: 0.0486 - val_acc: 0.9868\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 210s 3ms/step - loss: 0.0064 - acc: 0.9985 - val_loss: 0.0487 - val_acc: 0.9869\n",
      "Epoch 29/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9986\n",
      "Epoch 00029: reducing learning rate to 3.2000003557186575e-06.\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.0060 - acc: 0.9986 - val_loss: 0.0486 - val_acc: 0.9870\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0061 - acc: 0.9986 - val_loss: 0.0486 - val_acc: 0.9870\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 210s 4ms/step - loss: 0.0063 - acc: 0.9986 - val_loss: 0.0486 - val_acc: 0.9870\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 210s 4ms/step - loss: 0.0060 - acc: 0.9986 - val_loss: 0.0487 - val_acc: 0.9870\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0064 - acc: 0.9985 - val_loss: 0.0487 - val_acc: 0.9869\n",
      "Epoch 34/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9985\n",
      "Epoch 00034: reducing learning rate to 6.400000529538374e-07.\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0063 - acc: 0.9986 - val_loss: 0.0487 - val_acc: 0.9870\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 212s 4ms/step - loss: 0.0059 - acc: 0.9987 - val_loss: 0.0487 - val_acc: 0.9870\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0059 - acc: 0.9986 - val_loss: 0.0487 - val_acc: 0.9871\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 210s 3ms/step - loss: 0.0061 - acc: 0.9986 - val_loss: 0.0487 - val_acc: 0.9871\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 216s 4ms/step - loss: 0.0059 - acc: 0.9986 - val_loss: 0.0487 - val_acc: 0.9871\n",
      "Epoch 39/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9987\n",
      "Epoch 00039: reducing learning rate to 1.280000105907675e-07.\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0059 - acc: 0.9988 - val_loss: 0.0487 - val_acc: 0.9871\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 215s 4ms/step - loss: 0.0061 - acc: 0.9985 - val_loss: 0.0487 - val_acc: 0.9871\n",
      "Test loss: 0.0486788884263\n",
      "Test accuracy: 0.9871\n",
      "Remaining time: 5 days 18 hours 35 minutes 08 seconds\n",
      "\n",
      "Training for activation tanh with optimizer Nadam\n",
      "------------------------------\n",
      "Experiment 0\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 223s 4ms/step - loss: 0.1760 - acc: 0.9487 - val_loss: 0.0908 - val_acc: 0.9710\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 210s 4ms/step - loss: 0.0878 - acc: 0.9745 - val_loss: 0.0770 - val_acc: 0.9777\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 202s 3ms/step - loss: 0.0720 - acc: 0.9784 - val_loss: 0.0985 - val_acc: 0.9724\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 0.0685 - acc: 0.9798 - val_loss: 0.0921 - val_acc: 0.9764\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 210s 3ms/step - loss: 0.0626 - acc: 0.9814 - val_loss: 0.1041 - val_acc: 0.9743\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 0.0572 - acc: 0.9834 - val_loss: 0.0886 - val_acc: 0.9784\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0566 - acc: 0.9837 - val_loss: 0.0987 - val_acc: 0.9772\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 210s 4ms/step - loss: 0.0515 - acc: 0.9855 - val_loss: 0.0933 - val_acc: 0.9796\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0508 - acc: 0.9860 - val_loss: 0.1090 - val_acc: 0.9788\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.0531 - acc: 0.9860 - val_loss: 0.1212 - val_acc: 0.9785\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 210s 4ms/step - loss: 0.0489 - acc: 0.9881 - val_loss: 0.1279 - val_acc: 0.9778\n",
      "Epoch 12/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9880\n",
      "Epoch 00012: reducing learning rate to 0.0004000000189989805.\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.0458 - acc: 0.9880 - val_loss: 0.1284 - val_acc: 0.9792\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0204 - acc: 0.9944 - val_loss: 0.1063 - val_acc: 0.9823\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0130 - acc: 0.9962 - val_loss: 0.1004 - val_acc: 0.9833\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0096 - acc: 0.9970 - val_loss: 0.1059 - val_acc: 0.9816\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0071 - acc: 0.9979 - val_loss: 0.1016 - val_acc: 0.9828\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 206s 3ms/step - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0991 - val_acc: 0.9821\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0081 - acc: 0.9976 - val_loss: 0.1041 - val_acc: 0.9835\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 202s 3ms/step - loss: 0.0077 - acc: 0.9977 - val_loss: 0.1059 - val_acc: 0.9825\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0070 - acc: 0.9978 - val_loss: 0.1025 - val_acc: 0.9835\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0067 - acc: 0.9977 - val_loss: 0.1076 - val_acc: 0.9824\n",
      "Epoch 22/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9979\n",
      "Epoch 00022: reducing learning rate to 8.000000379979611e-05.\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0064 - acc: 0.9979 - val_loss: 0.1068 - val_acc: 0.9823\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 0.0042 - acc: 0.9990 - val_loss: 0.1014 - val_acc: 0.9839\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 210s 3ms/step - loss: 0.0038 - acc: 0.9990 - val_loss: 0.1029 - val_acc: 0.9836\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 206s 3ms/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.1018 - val_acc: 0.9831\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.1030 - val_acc: 0.9825\n",
      "Epoch 27/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9990\n",
      "Epoch 00027: reducing learning rate to 1.6000001050997525e-05.\n",
      "60000/60000 [==============================] - 210s 3ms/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.1037 - val_acc: 0.9827\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.0028 - acc: 0.9993 - val_loss: 0.1032 - val_acc: 0.9829\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0030 - acc: 0.9991 - val_loss: 0.1033 - val_acc: 0.9827\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 210s 4ms/step - loss: 0.0029 - acc: 0.9993 - val_loss: 0.1032 - val_acc: 0.9825\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.1032 - val_acc: 0.9830\n",
      "Epoch 32/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9992\n",
      "Epoch 00032: reducing learning rate to 3.2000003557186575e-06.\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.1035 - val_acc: 0.9830\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.1033 - val_acc: 0.9829\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 210s 3ms/step - loss: 0.0029 - acc: 0.9992 - val_loss: 0.1031 - val_acc: 0.9829\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 198s 3ms/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.1031 - val_acc: 0.9827\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 224s 4ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.1030 - val_acc: 0.9829\n",
      "Epoch 37/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9993\n",
      "Epoch 00037: reducing learning rate to 6.400000529538374e-07.\n",
      "60000/60000 [==============================] - 211s 4ms/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.1033 - val_acc: 0.9830\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.1032 - val_acc: 0.9830\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0028 - acc: 0.9993 - val_loss: 0.1031 - val_acc: 0.9830\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 210s 4ms/step - loss: 0.0027 - acc: 0.9993 - val_loss: 0.1032 - val_acc: 0.9831\n",
      "Test loss: 0.103151865698\n",
      "Test accuracy: 0.9831\n",
      "Remaining time: 5 days 15 hours 06 minutes 28 seconds\n",
      "\n",
      "Training for activation relu with optimizer rmsp\n",
      "------------------------------\n",
      "Experiment 0\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.1532 - acc: 0.9532 - val_loss: 0.0656 - val_acc: 0.9801\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.0597 - acc: 0.9826 - val_loss: 0.0444 - val_acc: 0.9858\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.0478 - acc: 0.9861 - val_loss: 0.0436 - val_acc: 0.9847\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.0404 - acc: 0.9880 - val_loss: 0.0389 - val_acc: 0.9870\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.0357 - acc: 0.9895 - val_loss: 0.0392 - val_acc: 0.9877\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 206s 3ms/step - loss: 0.0327 - acc: 0.9901 - val_loss: 0.0361 - val_acc: 0.9883\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0302 - acc: 0.9912 - val_loss: 0.0372 - val_acc: 0.9884\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.0284 - acc: 0.9917 - val_loss: 0.0366 - val_acc: 0.9883\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0270 - acc: 0.9923 - val_loss: 0.0365 - val_acc: 0.9898\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0263 - acc: 0.9925 - val_loss: 0.0336 - val_acc: 0.9900\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0241 - acc: 0.9927 - val_loss: 0.0349 - val_acc: 0.9892\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0235 - acc: 0.9932 - val_loss: 0.0507 - val_acc: 0.9850\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0230 - acc: 0.9933 - val_loss: 0.0377 - val_acc: 0.9892\n",
      "Epoch 14/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9937\n",
      "Epoch 00014: reducing learning rate to 0.00020000000949949026.\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.0217 - acc: 0.9937 - val_loss: 0.0340 - val_acc: 0.9900\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 202s 3ms/step - loss: 0.0157 - acc: 0.9955 - val_loss: 0.0329 - val_acc: 0.9908\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0140 - acc: 0.9959 - val_loss: 0.0336 - val_acc: 0.9911\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 202s 3ms/step - loss: 0.0140 - acc: 0.9961 - val_loss: 0.0342 - val_acc: 0.9908\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.0135 - acc: 0.9961 - val_loss: 0.0364 - val_acc: 0.9906\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0140 - acc: 0.9958 - val_loss: 0.0343 - val_acc: 0.9906\n",
      "Epoch 20/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9961\n",
      "Epoch 00020: reducing learning rate to 4.0000001899898055e-05.\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0131 - acc: 0.9961 - val_loss: 0.0355 - val_acc: 0.9905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0117 - acc: 0.9968 - val_loss: 0.0344 - val_acc: 0.9910\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0118 - acc: 0.9966 - val_loss: 0.0344 - val_acc: 0.9911\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0118 - acc: 0.9968 - val_loss: 0.0340 - val_acc: 0.9910\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.0117 - acc: 0.9967 - val_loss: 0.0340 - val_acc: 0.9911\n",
      "Epoch 25/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9968\n",
      "Epoch 00025: reducing learning rate to 8.000000525498762e-06.\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0117 - acc: 0.9968 - val_loss: 0.0344 - val_acc: 0.9908\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 206s 3ms/step - loss: 0.0114 - acc: 0.9967 - val_loss: 0.0344 - val_acc: 0.9908\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.0118 - acc: 0.9967 - val_loss: 0.0343 - val_acc: 0.9908\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 206s 3ms/step - loss: 0.0113 - acc: 0.9969 - val_loss: 0.0343 - val_acc: 0.9910\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0114 - acc: 0.9967 - val_loss: 0.0343 - val_acc: 0.9909\n",
      "Epoch 30/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9969\n",
      "Epoch 00030: reducing learning rate to 1.6000001778593287e-06.\n",
      "60000/60000 [==============================] - 197s 3ms/step - loss: 0.0114 - acc: 0.9969 - val_loss: 0.0342 - val_acc: 0.9909\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 217s 4ms/step - loss: 0.0111 - acc: 0.9969 - val_loss: 0.0342 - val_acc: 0.9910\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 226s 4ms/step - loss: 0.0116 - acc: 0.9967 - val_loss: 0.0342 - val_acc: 0.9910\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 202s 3ms/step - loss: 0.0114 - acc: 0.9967 - val_loss: 0.0342 - val_acc: 0.9910\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 202s 3ms/step - loss: 0.0116 - acc: 0.9966 - val_loss: 0.0342 - val_acc: 0.9910\n",
      "Epoch 35/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9969\n",
      "Epoch 00035: reducing learning rate to 3.200000264769187e-07.\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0110 - acc: 0.9969 - val_loss: 0.0343 - val_acc: 0.9910\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 0.0116 - acc: 0.9966 - val_loss: 0.0343 - val_acc: 0.9910\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0114 - acc: 0.9968 - val_loss: 0.0343 - val_acc: 0.9910\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0112 - acc: 0.9969 - val_loss: 0.0342 - val_acc: 0.9910\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0110 - acc: 0.9969 - val_loss: 0.0342 - val_acc: 0.9910\n",
      "Epoch 40/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9969\n",
      "Epoch 00040: reducing learning rate to 6.400000529538374e-08.\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.0114 - acc: 0.9969 - val_loss: 0.0342 - val_acc: 0.9910\n",
      "Test loss: 0.0342426698056\n",
      "Test accuracy: 0.991\n",
      "Remaining time: 5 days 11 hours 46 minutes 53 seconds\n",
      "\n",
      "Training for activation relu with optimizer adam\n",
      "------------------------------\n",
      "Experiment 0\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.1637 - acc: 0.9509 - val_loss: 0.0530 - val_acc: 0.9835\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0564 - acc: 0.9825 - val_loss: 0.0401 - val_acc: 0.9865\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.0418 - acc: 0.9871 - val_loss: 0.0397 - val_acc: 0.9876\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.0332 - acc: 0.9897 - val_loss: 0.0377 - val_acc: 0.9881\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0269 - acc: 0.9913 - val_loss: 0.0355 - val_acc: 0.9881\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0227 - acc: 0.9926 - val_loss: 0.0356 - val_acc: 0.9896\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0186 - acc: 0.9938 - val_loss: 0.0382 - val_acc: 0.9893\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0154 - acc: 0.9951 - val_loss: 0.0387 - val_acc: 0.9894\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.0141 - acc: 0.9954 - val_loss: 0.0432 - val_acc: 0.9878\n",
      "Epoch 10/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9957\n",
      "Epoch 00010: reducing learning rate to 0.00020000000949949026.\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0126 - acc: 0.9958 - val_loss: 0.0447 - val_acc: 0.9881\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0049 - acc: 0.9986 - val_loss: 0.0390 - val_acc: 0.9897\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 202s 3ms/step - loss: 0.0036 - acc: 0.9991 - val_loss: 0.0388 - val_acc: 0.9897\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 202s 3ms/step - loss: 0.0029 - acc: 0.9993 - val_loss: 0.0425 - val_acc: 0.9898\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.0449 - val_acc: 0.9900\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.0442 - val_acc: 0.9897\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 202s 3ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0466 - val_acc: 0.9894\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 209s 3ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0449 - val_acc: 0.9899\n",
      "Epoch 18/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 00018: reducing learning rate to 4.0000001899898055e-05.\n",
      "60000/60000 [==============================] - 207s 3ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0463 - val_acc: 0.9901\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0462 - val_acc: 0.9896\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 227s 4ms/step - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0467 - val_acc: 0.9898\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 239s 4ms/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0468 - val_acc: 0.9898\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 231s 4ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0457 - val_acc: 0.9905\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 240s 4ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0468 - val_acc: 0.9900\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 231s 4ms/step - loss: 9.8273e-04 - acc: 0.9999 - val_loss: 0.0471 - val_acc: 0.9899\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 233s 4ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0470 - val_acc: 0.9899\n",
      "Epoch 26/40\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 9.2214e-04 - acc: 0.9999\n",
      "Epoch 00026: reducing learning rate to 8.000000525498762e-06.\n",
      "60000/60000 [==============================] - 250s 4ms/step - loss: 9.2187e-04 - acc: 0.9999 - val_loss: 0.0476 - val_acc: 0.9902\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 260s 4ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0470 - val_acc: 0.9903\n",
      "Epoch 28/40\n",
      "53120/60000 [=========================>....] - ETA: 28s - loss: 9.3457e-04 - acc: 0.9999"
     ]
    }
   ],
   "source": [
    "\n",
    "for act in activations:\n",
    "    for opt in optimizers:\n",
    "        print(\"\\nTraining for activation \" + act + \" with optimizer \" + opt)\n",
    "\n",
    "        # Selecting activation function\n",
    "        if act == 'sigmoid':\n",
    "            activation = Activation(keras.activations.sigmoid)\n",
    "        elif act == 'tanh':\n",
    "            activation = Activation(keras.activations.tanh)\n",
    "        elif act == 'relu':\n",
    "            activation = Activation(keras.activations.relu)\n",
    "        elif act == 'linear':\n",
    "            activation = Activation(keras.activations.linear)\n",
    "        elif act == 'elu':\n",
    "            activation = Activation(keras.activations.elu)\n",
    "        elif act == 'softplus':\n",
    "            activation = Activation(keras.activations.softplus)\n",
    "        elif act == 'softsign':\n",
    "            activation = Activation(keras.activations.softsign)\n",
    "        elif act == 'hard_sigmoid':\n",
    "            activation = Activation(keras.activations.hard_sigmoid)\n",
    "        elif act == 'LeakyReLU':\n",
    "            activation = keras.layers.advanced_activations.LeakyReLU()\n",
    "        elif act == 'PReLU':\n",
    "            activation = keras.layers.advanced_activations.PReLU()\n",
    "        elif act == 'ThresholdedReLU':\n",
    "            activation = keras.layers.advanced_activations.ThresholdedReLU(theta=0.7)\n",
    "            \n",
    "\n",
    "        if opt == 'rmsp':\n",
    "            optimizer = keras.optimizers.rmsprop()                \n",
    "        elif opt == 'adam':\n",
    "            optimizer = keras.optimizers.Adam()\n",
    "        elif opt == 'sgd':\n",
    "            optimizer = keras.optimizers.SGD()\n",
    "        elif opt == 'Adagrad':\n",
    "            optimizer = keras.optimizers.Adagrad()\n",
    "        elif opt == 'Adadelta':\n",
    "            optimizer = keras.optimizers.Adadelta()\n",
    "        elif opt == 'Adamax':\n",
    "            optimizer = keras.optimizers.Adamax()\n",
    "        elif opt == 'Nadam':\n",
    "            optimizer = keras.optimizers.Nadam()\n",
    "        \n",
    "        for i in range(experiments):\n",
    "            if add_final_dense:\n",
    "                model_name = 'mnist_cnn_' + act + \"_\" + opt + '_' + str(epochs) + '_' +str(i + start) + '_fd'\n",
    "            else:\n",
    "                model_name = 'mnist_cnn_' + act + \"_\" + opt + '_' + str(epochs) + '_' + str(i + start)\n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                             input_shape=input_shape))\n",
    "            \n",
    "            model.add(activation)\n",
    "            model.add(Conv2D(64, kernel_size=(3, 3)))\n",
    "            model.add(activation)\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same'))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Flatten())\n",
    "            if add_final_dense:\n",
    "                model.add(Dense(128))\n",
    "                model.add(activation)\n",
    "                model.add(Dropout(0.2))\n",
    "            model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                              optimizer=optimizer,\n",
    "                              metrics=['accuracy'])\n",
    "            \n",
    "            print('-'*30)\n",
    "            print('Experiment', i)\n",
    "\n",
    "            csv_logger = CSVLogger('./logs/%s_%d.csv' % (model_name, units), append=False, separator=';')\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=3, verbose=1, mode='auto', epsilon=0.0001, cooldown=2, min_lr=0)\n",
    "#             tb = TensorBoard(log_dir='./tb_logs/' + model_name + '_' + str(units), histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "            history = model.fit(x_train, y_train,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=epochs,\n",
    "                                verbose=1,\n",
    "                                validation_data=(x_test, y_test), callbacks=[csv_logger, reduce_lr])\n",
    "\n",
    "            if not os.path.isdir(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            model_path = os.path.join(save_dir, model_name + \".h5\")\n",
    "#             model.save(model_path)\n",
    "\n",
    "            score = model.evaluate(x_test, y_test, verbose=0)\n",
    "            print('Test loss:', score[0])\n",
    "            print('Test accuracy:', score[1])\n",
    "            t = time.time()\n",
    "            time_diff = t - start_time\n",
    "            counter +=1\n",
    "            rem_items = total_items - counter\n",
    "            total_time = round((total_items / counter) * time_diff)\n",
    "            rem_time = round(total_time - time_diff)\n",
    "            m, s = divmod(rem_time, 60)\n",
    "            h, m = divmod(m, 60)\n",
    "            d, h = divmod(h, 24)\n",
    "            print('Remaining time: %d days %d hours %02d minutes %02d seconds' % (d, h, m, s))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeLU\n",
    "This one has special requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = 2\n",
    "for opt in optimizers:\n",
    "    print(\"Training for activation SeLU with optimizer \" + opt)\n",
    "    for i in range(experiments):\n",
    "        if add_final_dense:\n",
    "            model_name = 'mnist_cnn_selu_' + opt + '_' + str(epochs) + '_' +str(i + start) + '_fd'\n",
    "        else:\n",
    "            model_name = 'mnist_cnn_selu_' + opt + '_' + str(epochs) + '_' + str(i + start)\n",
    "                \n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                         activation='selu', kernel_initializer='lecun_normal',\n",
    "                         input_shape=input_shape))\n",
    "        model.add(Conv2D(64, (3, 3), activation='selu', kernel_initializer='lecun_normal'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(AlphaDropout(0.2))\n",
    "        model.add(Flatten())\n",
    "        if add_final_dense:\n",
    "            model.add(Dense(128, activation='selu', kernel_initializer='lecun_normal'))\n",
    "            model.add(AlphaDropout(0.2))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        if opt == 'rmsp':\n",
    "            optimizer = keras.optimizers.rmsprop()                \n",
    "        elif opt == 'adam':\n",
    "            optimizer = keras.optimizers.Adam()\n",
    "        elif opt == 'sgd':\n",
    "            optimizer = keras.optimizers.SGD()\n",
    "        elif opt == 'Adagrad':\n",
    "            optimizer = keras.optimizers.Adagrad()\n",
    "        elif opt == 'Adadelta':\n",
    "            optimizer = keras.optimizers.Adadelta()\n",
    "        elif opt == 'Adamax':\n",
    "            optimizer = keras.optimizers.Adamax()\n",
    "        elif opt == 'Nadam':\n",
    "            optimizer = keras.optimizers.Nadam()\n",
    "\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=['accuracy'])\n",
    "        print('-'*30)\n",
    "        print('Experiment', i+1)\n",
    "\n",
    "        csv_logger = CSVLogger('./logs/%s_%d.csv' % (model_name, units), append=False, separator=';')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=3, verbose=0, mode='auto', epsilon=0.0001, cooldown=2, min_lr=0)\n",
    "#         tb = TensorBoard(log_dir='./tb_logs/' + model_name + '_' + str(units), histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "        history = model.fit(x_train, y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            validation_data=(x_test, y_test), callbacks=[csv_logger, reduce_lr])\n",
    "\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        model_path = os.path.join(save_dir, model_name + \".h5\")\n",
    "#         model.save(model_path)\n",
    "        score = model.evaluate(x_test, y_test, verbose=0)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "        t = time.time()\n",
    "        counter +=1\n",
    "        time_diff = t - start_time\n",
    "        rem_items = total_items - counter\n",
    "        total_time = round(total_items / counter) * time_diff)\n",
    "        rem_time = round(total_time - time_diff)\n",
    "        m, s = divmod(rem_time, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        d, h = divmod(h, 24)\n",
    "        print('Remaining time: %d days %d hours %02d minutes %02d seconds' % (d, h, m, s))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
